### æ„è§æ€§åˆ†äº«

#### How to store 3D scenes/grids efficiently? 

[Matthias Niessner](https://twitter.com/MattNiessner)

(1/2)
Just hash occupied voxels, ğ»(ğ‘¥,ğ‘¦,ğ‘§)=(ğ‘¥â‹…ğ‘_1â¨ ğ‘¦â‹…ğ‘_2â¨ ğ‘§â‹…ğ‘_3) mod ğ‘›, and store them in a linear array!

The voxel hashing method proposed this for 3D recon in 2013, but it's used in many apps such as sparse conv nets!

<div align=center><img src="https://pbs.twimg.com/media/FGl0WSmWUAAWwAL?format=png&name=900x900" alt="Cover" width="120"/></div>

(2/2)
To access the value of a voxel, simply evaluate the hash, e.g., for a filter kernel or to update a TSDF value.

Why are hashes so powerful? 

Data access *and* insertion is O(1) - ignoring collisions. In comparison, for tree structures, such as octrees, that's O(log(n)).

[Andreas Kirsch] 
Wasn't that a paper by @sylefeb, too?https://t.co/SUwJlGNxcY

[Sylvain Lefebvre]
Thanks for the mention! We have some other works on the topic too (e.g. showing that coalesced accesses are possible through a hash on the GPU: https://hal.inria.fr/inria-00624777/document  )

[game programmer dude]
What do you with dynamic objects moving through the grids? Do you need to remove them from the voxel they were in and then add them to the voxel they move into? Seems like removal of an object from one voxel takes o(n).

[Morten Vassvik]
I can't find a list of the bounding box spans of all the test cases in the paper. In most cases at least one dimension is missing, e.g. Figure 1 doesn't mention the depth, so I'm not able to compute the efficient sparsity and span of each case. Are these available somewhere? :)

### è¯¾ç¨‹å’ŒæŠ¥å‘Šåˆ†äº«

#### Physical reasoning and inductive biases

Happening tomorrow! #NeurIPS 2021 workshop on physical reasoning and inductive biases for the real world!

Join us at 11 am EST (8 am PST, 4 pm GMT/UTC)

For schedule and accepted papers, see https://physical-reasoning.github.io

Fantastic speakers and panelists from industry/academia

<div align=center><img src="https://pbs.twimg.com/media/FGhZigpXoAQwqVm?format=jpg&name=medium" alt="Cover" width="75%"/></div>

reference: https://twitter.com/_krishna_murthy/status/1470524128922849282?s=20


***

### æˆæœæ¨èåŠè®¨è®º

- ##### Hallucinating Pose-Compatible Scenes

  [AK](https://twitter.com/ak92501)
  
  abs: https://arxiv.org/abs/2112.06909
  
  <div align=center><img src="https://pbs.twimg.com/media/FGiVHfcWUAcqRO5?format=jpg&name=medium" alt="Cover" width="60%"/></div>
  
  almost think the non compatible combinations are more promising for art
  
  <div align=center><img src="https://pbs.twimg.com/media/FGlvgIpVUAMrqHl?format=jpg&name=medium" alt="Cover" width="60%"/></div>
  
  reference: https://twitter.com/ak92501/status/1470589245874180096?s=20
  
- ##### ONE MORE POLY

  [EveryPoint](https://twitter.com/EveryPointIO)
  
  We collaborated with Juan Murphy at ONE MORE POLY. His vision: bridging physical + digital content to give creative agencies and brands a new opportunity to create immersive AR and VR content.
  
  reference: https://twitter.com/EveryPointIO/status/1470541263040487424?s=20
  
- #####  repulsive shape optimization

  [Keenan Crane](https://twitter.com/keenanisalive)
  
   Short news story about our work on repulsive shape optimization:https://scs.cmu.edu/news/2021/repulsive-energies

   This is joint work with recent @SCSatCMU rad Chris Yu, Henrik Schumacher from RWTH Aachen University/TU Chemnitz, and high school student Caleb Brakensiek.
   
   homepage:https://www.scs.cmu.edu/news/2021/repulsive-energies

    <div align=center><img src="https://www.scs.cmu.edu/files/images/repulsive-shapes.png" alt="Cover" width="60%"/></div>
    
- #####  Equivariant Subgraph Aggregation Networks
  
  [Hannes StÃ¤rk](https://twitter.com/HannesStaerk)
  
  This Tuesday, we'll discuss the paper "Equivariant Subgraph Aggregation Networks" with the authors.
  
  Join us on Zoom at 4pm GMT: https://hannes-stark.com/logag-reading-group
  
  They propose the interesting idea to leverage that we can get more expressive GNNs if we consider perturbations of the original graph in addition to the original graph.
     <div align=center><img src="https://pbs.twimg.com/media/FGf9p0xX0AM3vRk?format=jpg&name=small" alt="Cover" width="60%"/></div>
     
- #####  Self-Calibrating Neural Radiance Fields

  [Andrea Tagliasacchi](https://twitter.com/taiyasaki)

  "Self-Calibrating Neural Radiance Fields"
  https://arxiv.org/abs/2108.13826

  Noteworthy :)
  
     <div align=center><img src="https://pbs.twimg.com/media/FGhw-uPVIAUdgN7?format=png&name=small" alt="Cover" width="60%"/></div>
